{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteracion 50: Recompensa maxima = 2.452807127963673\n",
      "Iteracion 100: Recompensa maxima = 4.417338614521361\n",
      "Iteracion 150: Recompensa maxima = 6.072889716421948\n",
      "Iteracion 200: Recompensa maxima = 7.209579114149409\n",
      "Iteracion 250: Recompensa maxima = 8.076146871060327\n",
      "Iteracion 300: Recompensa maxima = 8.725866623212415\n",
      "Iteracion 350: Recompensa maxima = 9.18122710947292\n",
      "Iteracion 400: Recompensa maxima = 9.479108570664105\n",
      "Iteracion 450: Recompensa maxima = 9.668616432841445\n",
      "Iteracion 500: Recompensa maxima = 9.791286798411114\n",
      "Iteracion 550: Recompensa maxima = 9.871163398374676\n",
      "Iteracion 600: Recompensa maxima = 9.922053074347298\n",
      "Iteracion 650: Recompensa maxima = 9.952841637065397\n",
      "Iteracion 700: Recompensa maxima = 9.971468904308292\n",
      "Iteracion 750: Recompensa maxima = 9.982738514004433\n",
      "Iteracion 800: Recompensa maxima = 9.989556696244867\n",
      "Iteracion 850: Recompensa maxima = 9.99368173786718\n",
      "Iteracion 900: Recompensa maxima = 9.996177413075875\n",
      "Iteracion 950: Recompensa maxima = 9.99768731171874\n",
      "Iteracion 1000: Recompensa maxima = 9.998600809558443\n",
      "Tabla Q final:\n",
      "[[-2.50177468 -2.49380934 -2.4951851  -0.05450279]\n",
      " [-1.75534934  1.83305914 -1.77759192 -1.74550208]\n",
      " [-1.09485366 -0.81707389 -1.09054466 -1.08842119]\n",
      " [-0.60558907 -0.26653687 -0.60757752 -0.60654181]\n",
      " [-0.32438418  0.11280006 -0.3952699  -0.34385184]\n",
      " [-1.78112671 -1.77281988 -1.77686145 -1.64136459]\n",
      " [-1.26066892 -1.24903583 -1.25145569  3.68943931]\n",
      " [-0.75425022 -0.71364933 -0.67485774  5.40088702]\n",
      " [-0.47271668 -0.26115822 -0.38794759  6.97529965]\n",
      " [-0.32428353  8.48109836 -0.16088307  0.02158417]\n",
      " [-1.24140971 -1.23092469 -1.23611905 -1.22982376]\n",
      " [-0.8156092  -0.79757716 -0.79617238 -0.45147598]\n",
      " [-0.47668432 -0.40516096 -0.54898463  1.71015405]\n",
      " [-0.38273211  0.15635974 -0.34675154  5.58963223]\n",
      " [-0.1438482   9.99860081 -0.08945944  0.52429608]\n",
      " [-0.89058608 -0.88721858 -0.88671818 -0.8845011 ]\n",
      " [-0.53338427 -0.55038281 -0.54050161 -0.039468  ]\n",
      " [-0.3844467  -0.42940742 -0.47480001  2.43946741]\n",
      " [-0.20907371  0.13282475 -0.29413776  6.91455529]\n",
      " [ 0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Definir el entorno\n",
    "n_rows = 4 # numero de filas\n",
    "n_cols = 5 # numero de columnas\n",
    "n_states = n_rows * n_cols # numero total de estados\n",
    "n_actions = 4  # numero total de acciones: arriba, abajo, izquierda y derecha\n",
    "\n",
    "# Hiperparametros\n",
    "learning_rate = 0.01 # determina el tamaño del paso en cada iteración mientras avanza\n",
    "discount_factor = 0.95 # determina la importancia de las recompensas futuras\n",
    "epsilon = 1.0 # tasa de exploracion inicial\n",
    "epsilon_decay_rate = 0.005 # es la velocidad a la que épsilon disminuye con el tiempo para reducir la exploración\n",
    "rng = np.random.default_rng() # es una instancia de generador de números aleatorios de NumPy, utilizada para generar números aleatorios\n",
    "\n",
    "# Inicializar la tabla Q\n",
    "q_table = np.zeros((n_states, n_actions))\n",
    "\n",
    "# Definir funciones auxiliares\n",
    "# Convierte una posición de cuadrícula (fila, columna) en un índice de estado\n",
    "def get_state(row, col):\n",
    "    return row * n_cols + col\n",
    "\n",
    "# Convierte un índice de estado nuevamente en coordenadas de cuadrícula (fila, columna)\n",
    "# mediante división de enteros y operaciones de módulo.\n",
    "def get_position(state):\n",
    "    return divmod(state, n_cols)\n",
    "\n",
    "# Comprueba si una posición determinada (fila, columna) está dentro de los límites de la cuadrícula.\n",
    "# Esto evita que el agente se mueva fuera de la red.\n",
    "def is_valid_position(row, col):\n",
    "    return 0 <= row < n_rows and 0 <= col < n_cols\n",
    "\n",
    "# Determina el siguiente estado dado el estado y la acción actuales.\n",
    "# Ajusta la fila o columna según la acción y comprueba si la nueva posición es válida.\n",
    "# Si el movimiento no es válido, el agente permanece en el mismo estado.\n",
    "def get_next_state(state, action):\n",
    "    row, col = get_position(state)\n",
    "    if action == 0:  # arriba\n",
    "        row -= 1\n",
    "    elif action == 1:  # abajo\n",
    "        row += 1\n",
    "    elif action == 2:  # izquierda\n",
    "        col -= 1\n",
    "    elif action == 3:  # derecha\n",
    "        col += 1\n",
    "    if is_valid_position(row, col):\n",
    "        return get_state(row, col)\n",
    "    else:\n",
    "        return state  # Si el movimiento no es válido, permanezca en el mismo estado.\n",
    "\n",
    "# Devuelve la recompensa por un estado determinado.\n",
    "# En este caso, alcanzar el estado objetivo (esquina inferior derecha) otorga una recompensa de 10,\n",
    "# mientras que cualquier otro estado otorga una penalización de -1.\n",
    "def get_reward(state):\n",
    "    # Definir la función de recompensa\n",
    "    # Suponiendo que el estado objetivo es la esquina inferior derecha\n",
    "    goal_state = n_states - 1\n",
    "    if state == goal_state:\n",
    "        return 10  # Recompensa por alcanzar la meta.\n",
    "    else:\n",
    "        return -1  # Penalización por cada paso.\n",
    "\n",
    "# Establece el número de iteraciones de entrenamiento\n",
    "n_iterations = 1000\n",
    "# inicializa una lista para almacenar las recompensas máximas observadas cada 100 iteraciones\n",
    "max_rewards = []\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    # Cada iteración comienza con el agente en la esquina superior izquierda (estado 0).\n",
    "    state = get_state(0, 0)\n",
    "    # total_rewardrastrea la recompensa acumulada para el episodio actual.\n",
    "    total_reward = 0\n",
    "\n",
    "    # Elige una acción basada en la estrategia épsilon-codiciosa\n",
    "    # Con probabilidad epsilon, el agente explora eligiendo una acción aleatoria.\n",
    "    # De lo contrario, el agente explota eligiendo la acción con el valor Q más alto para el estado actual.\n",
    "    while True:\n",
    "        if rng.random() < epsilon:  # Explorar\n",
    "            action = rng.integers(n_actions)\n",
    "        else:  # Explotar\n",
    "            action = np.argmax(q_table[state])\n",
    "\n",
    "        # Ejecuta la acción elegida:\n",
    "        # new_state se determina en función del estado actual y la acción.\n",
    "        # reward se obtiene por pasar al nuevo estado.\n",
    "        # total_reward se actualiza con la recompensa recibida.\n",
    "        new_state = get_next_state(state, action)\n",
    "        reward = get_reward(new_state)\n",
    "        total_reward += reward\n",
    "\n",
    "        # Actualizar tabla Q\n",
    "        q_table[state, action] = q_table[state, action] + learning_rate * (\n",
    "            reward + discount_factor * np.max(q_table[new_state]) - q_table[state, action]\n",
    "        )\n",
    "        \n",
    "        # Actualiza el estado actual al nuevo estado.\n",
    "        state = new_state\n",
    "\n",
    "        # Comprueba si se alcanza el estado objetivo (esquina inferior derecha). Si es así, el episodio termina.\n",
    "        if state == get_state(n_rows - 1, n_cols - 1):\n",
    "            break\n",
    "\n",
    "    # Disminuye la tasa de exploración epsilondespués de cada episodio para pasar gradualmente\n",
    "    # de exploración a explotación. Garantiza epsilon que no caiga por debajo de 0,01.\n",
    "    epsilon = max(epsilon - epsilon_decay_rate, 0.01)\n",
    "    \n",
    "    # Imprimir resultados cada 100 iteraciones\n",
    "    if (iteration + 1) % 50 == 0:\n",
    "        max_reward = np.max(q_table)\n",
    "        print(f\"Iteracion {iteration + 1}: Recompensa maxima = {max_reward}\")\n",
    "        max_rewards.append(max_reward)\n",
    "\n",
    "# Tabla Q final\n",
    "print(\"Tabla Q final:\")\n",
    "print(q_table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
